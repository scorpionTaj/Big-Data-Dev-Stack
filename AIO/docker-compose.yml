version: "3"

networks:
  bigdata-net:
    driver: bridge

services:
  # ===========================================================================
  # CORE INFRASTRUCTURE (Coordination & Storage)
  # ===========================================================================

  # ZOOKEEPER: The "Coordinator"
  # Used by HBase, Kafka, and High-Availability clusters to manage state.
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    restart: always
    ports: ["2181:2181"]
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks: [bigdata-net]

  # NAMENODE: HDFS Master
  # Stores metadata about where files are stored across the cluster.
  # UI: http://localhost:9870
  namenode:
    image: apache/hadoop:3
    container_name: namenode
    hostname: namenode
    restart: always
    command: ["hdfs", "namenode"]
    ports: ["9870:9870", "9000:9000"]
    environment:
      - HDFS_CONF_dfs_namenode_name_dir=file:///hadoop/dfs/name
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks: [bigdata-net]

  # DATANODE: HDFS Worker
  # The actual storage node where data blocks are kept.
  datanode:
    image: apache/hadoop:3
    container_name: datanode
    restart: always
    command: ["hdfs", "datanode"]
    environment:
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks: [bigdata-net]

  # RESOURCEMANAGER: YARN Master
  # Manages resources (CPU/RAM) for jobs (MapReduce, Spark, etc.).
  # UI: http://localhost:8088
  resourcemanager:
    image: apache/hadoop:3
    container_name: resourcemanager
    restart: always
    command: ["yarn", "resourcemanager"]
    ports: ["8088:8088"]
    environment:
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks: [bigdata-net]

  # NODEMANAGER: YARN Worker
  # Executes the actual tasks scheduled by the ResourceManager.
  nodemanager:
    image: apache/hadoop:3
    container_name: nodemanager
    restart: always
    command: ["yarn", "nodemanager"]
    environment:
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks: [bigdata-net]

  # ===========================================================================
  # DATA INGESTION & MESSAGING
  # ===========================================================================

  # KAFKA: Message Broker
  # Handles real-time data streaming.
  # Ports: 9092 (External access), 9093 (Internal Docker access).
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka
    restart: always
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    depends_on: [zookeeper]
    networks: [bigdata-net]

  # KAFKA CONNECT: Replaces Flume
  # Used for connecting Kafka to HDFS, Databases, etc.
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.6.0
    container_name: kafka-connect
    ports: ["8083:8083"]
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "compose-connect-group"
      CONNECT_CONFIG_STORAGE_TOPIC: "docker-connect-configs"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: "docker-connect-offsets"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: "docker-connect-status"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    depends_on: [kafka]
    networks: [bigdata-net]

  # ===========================================================================
  # SQL & WAREHOUSING (Hive)
  # ===========================================================================

  # HIVE SERVER: SQL Interface
  # Allows you to query HDFS data using SQL (JDBC/ODBC).
  # JDBC URL: jdbc:hive2://localhost:10000
  hive-server:
    image: apache/hive:3.1.3
    container_name: hive-server
    restart: always
    ports: ["10000:10000", "10002:10002"]
    environment:
      - SERVICE_NAME=hiveserver2
      - HIVE_DB_EXTERNAL=true
      - HIVE_DB_HOST=hive-postgres
      - HIVE_DB_PORT=5432
      - HIVE_DB_NAME=metastore
      - HIVE_DB_USER=hive
      - HIVE_DB_PASSWORD=hive
    depends_on: [namenode, hive-metastore]
    networks: [bigdata-net]

  # HIVE METASTORE: Schema Registry
  # Stores table definitions (columns, types, locations).
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    restart: always
    environment:
      - SERVICE_NAME=metastore
      - HIVE_DB_EXTERNAL=true
      - HIVE_DB_HOST=hive-postgres
      - HIVE_DB_PORT=5432
      - HIVE_DB_NAME=metastore
      - HIVE_DB_USER=hive
      - HIVE_DB_PASSWORD=hive
    depends_on: [hive-postgres]
    networks: [bigdata-net]

  # POSTGRES: Metastore Database
  # The actual database backend for Hive Metastore (stores schemas, not data).
  hive-postgres:
    image: postgres:13
    container_name: hive-postgres
    restart: always
    environment:
      - POSTGRES_DB=metastore
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
    networks: [bigdata-net]

  # ===========================================================================
  # DATA PROCESSING (Spark + Jupyter)
  # ===========================================================================

  # SPARK MASTER: Cluster Manager
  # Manages Spark applications and schedules tasks on workers.
  # UI: http://localhost:8080
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    # Custom command to launch the master class in foreground
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports: ["8080:8080", "7077:7077"]
    networks: [bigdata-net]

  # SPARK WORKER: Processing Node
  # Executes Spark tasks (RDDs/DataFrames) and reports to Master.
  # UI: http://localhost:8081
  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    # Custom command to launch worker and connect to master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    ports: ["8081:8081"]
    depends_on: [spark-master]
    networks: [bigdata-net]

  # JUPYTER LAB: Replaces "spark-client"
  # UI accessible at http://localhost:8888 (Password in logs)
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.5.0
    container_name: jupyter
    ports: ["8888:8888"]
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      - ./notebooks:/home/scorpiontaj/work
    networks: [bigdata-net]

  # ===========================================================================
  # NoSQL DATABASES (HBase, Cassandra, Neo4j)
  # ===========================================================================

  # HBASE MASTER: Column-Store Master
  # Coordinates the HBase cluster.
  # UI: http://localhost:16010
  hbase-master:
    image: harisekhon/hbase:2.1
    container_name: hbase-master
    ports: ["16010:16010"]
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_CONF_hbase_rootdir=hdfs://namenode:9000/hbase
      - HBASE_CONF_hbase_zookeeper_quorum=zookeeper
    depends_on: [namenode, zookeeper]
    networks: [bigdata-net]

  # HBASE REGIONSERVER: Column-Store Worker
  # Stores and manages the actual HBase data regions.
  hbase-regionserver:
    image: harisekhon/hbase:2.1
    container_name: hbase-regionserver
    environment:
      - HBASE_MANAGES_ZK=false
      - HBASE_CONF_hbase_rootdir=hdfs://namenode:9000/hbase
      - HBASE_CONF_hbase_zookeeper_quorum=zookeeper
    depends_on: [hbase-master]
    networks: [bigdata-net]

  # CASSANDRA: Wide-Column Store
  # Standalone NoSQL database for high-availability apps.
  # Port: 9042 (CQL)
  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    restart: always
    ports: ["9042:9042"]
    environment:
      - CASSANDRA_CLUSTER_NAME=DevCluster
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
    networks: [bigdata-net]

  # NEO4J: Graph Database
  # Stores data as nodes and relationships.
  # UI: http://localhost:7474
  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j
    restart: always
    ports: ["7474:7474", "7687:7687"]
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc"]
    networks: [bigdata-net]

  # OOZIE: Dropping Oozie is recommended in modern stacks

  # HUE: Hadoop User Experience
  # Web UI for browsing HDFS, Hive Editor, and Job Browser.
  # UI: http://localhost:8888
  hue:
    image: gethue/hue:4.10.0
    container_name: hue
    ports: ["8888:8888"]
    environment:
      - HUE_NAMENODE_HOST=namenode
      - HUE_HDFS_DEFAULT_FS=hdfs://namenode:9000
      - HUE_RM_HOST=resourcemanager
      - HUE_RM_PORT=8088
    depends_on: [namenode, resourcemanager, hive-server]
    networks: [bigdata-net]

volumes:
  namenode_data:
  datanode_data:
  zookeeper_data:
  zookeeper_datalog:
  hive_postgres_data:
  cassandra_data:
  neo4j_data:
  neo4j_logs:
