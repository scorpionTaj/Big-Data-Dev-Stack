# ğŸ˜ Local Big Data Dev Stack

<p align="center">
  <img src="https://img.shields.io/badge/Hadoop-3.2.1-yellow?style=for-the-badge&logo=apache-hadoop&logoColor=white" alt="Hadoop"/>
  <img src="https://img.shields.io/badge/Spark-3.0.0-orange?style=for-the-badge&logo=apache-spark&logoColor=white" alt="Spark"/>
  <img src="https://img.shields.io/badge/Hive-2.3.2-green?style=for-the-badge&logo=apache-hive&logoColor=white" alt="Hive"/>
  <img src="https://img.shields.io/badge/HBase-1.2.6-red?style=for-the-badge&logo=apache&logoColor=white" alt="HBase"/>
  <img src="https://img.shields.io/badge/Cassandra-4.0-blue?style=for-the-badge&logo=apache-cassandra&logoColor=white" alt="Cassandra"/>
</p>

<p align="center">
  <strong>A complete Big Data ecosystem orchestrated with Docker Compose</strong><br>
  <em>Hadoop â€¢ Hive â€¢ HBase â€¢ Spark â€¢ Cassandra</em>
</p>

---

## ğŸ“‹ Table of Contents

- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Architecture](#-architecture)
- [Access Points & UIs](#-access-points--uis)
- [CLI Access](#-cli-access)
- [Configuration Notes](#-configuration-notes)
- [Troubleshooting](#-troubleshooting)

---

## âš ï¸ Prerequisites

| Requirement       | Minimum  | Recommended |
| ----------------- | -------- | ----------- |
| ğŸ³ Docker         | Latest   | Latest      |
| ğŸ”§ Docker Compose | v2.0+    | v2.0+       |
| ğŸ’¾ RAM            | 10-12 GB | 16 GB       |
| ğŸ–¥ï¸ CPUs           | 4 cores  | 6+ cores    |

> **ğŸ’¡ Tip:** If you have less than 16GB RAM, consider commenting out services in `docker-compose.yml` that you aren't currently using (e.g., disable Cassandra if you're only working with Hive).

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Start the cluster

```bash
docker-compose up -d
```

### 2ï¸âƒ£ Wait for initialization

â³ Allow **2-3 minutes** for all services to initialize:

- Hadoop SafeMode exit
- Hive Metastore schema initialization
- HBase region assignment

### 3ï¸âƒ£ Verify services

```bash
docker-compose ps
```

### ğŸ›‘ Stop the cluster

```bash
# Stop containers (preserves data)
docker-compose down

# Stop and remove all data (âš ï¸ destructive)
docker-compose down -v
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        bigdata-net (Docker Network)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Zookeeper   â”‚     â”‚   NameNode   â”‚     â”‚   DataNode   â”‚     â”‚
â”‚  â”‚    :2181     â”‚     â”‚    :9870     â”‚     â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚         â–¼                    â–¼                    â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ HBase Master â”‚     â”‚           HDFS Cluster          â”‚       â”‚
â”‚  â”‚   :16010     â”‚â—„â”€â”€â”€â”€â”‚                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                           â”‚                            â”‚
â”‚         â–¼                           â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   HBase      â”‚     â”‚    Hive      â”‚     â”‚    Spark     â”‚     â”‚
â”‚  â”‚ RegionServer â”‚     â”‚   Server     â”‚     â”‚   Master     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   :10000     â”‚     â”‚    :8080     â”‚     â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                    â”‚              â”‚
â”‚                              â–¼                    â–¼              â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                       â”‚  PostgreSQL  â”‚     â”‚    Spark     â”‚     â”‚
â”‚                       â”‚  (Metastore) â”‚     â”‚   Worker     â”‚     â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    :8081     â”‚     â”‚
â”‚                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Cassandra   â”‚  (Standalone NoSQL)                           â”‚
â”‚  â”‚    :9042     â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ Access Points & UIs

### ğŸŒ Web Interfaces

|   Service    |  Component  |                       URL                        | Description                 |
| :----------: | :---------: | :----------------------------------------------: | :-------------------------- |
| ğŸ—‚ï¸ **HDFS**  | NameNode UI |  [http://localhost:9870](http://localhost:9870)  | Browse the file system      |
| âš¡ **Spark** |  Master UI  |  [http://localhost:8080](http://localhost:8080)  | View Spark jobs & workers   |
| âš¡ **Spark** |  Worker UI  |  [http://localhost:8081](http://localhost:8081)  | View worker details         |
| ğŸ“Š **HBase** |  Master UI  | [http://localhost:16010](http://localhost:16010) | View HBase tables & regions |

### ğŸ”— Connection Ports

|     Service      |  Port   | Protocol | Connect With               |
| :--------------: | :-----: | :------: | :------------------------- |
|   ğŸ **Hive**    | `10000` |   JDBC   | Beeline, DBeaver, DataGrip |
|   ğŸ **Hive**    | `10002` |   HTTP   | Web UI                     |
| ğŸ”µ **Cassandra** | `9042`  |   CQL    | cqlsh, drivers             |
|   ğŸ—‚ï¸ **HDFS**    | `9000`  |   RPC    | Hadoop clients             |
|   âš¡ **Spark**   | `7077`  |   RPC    | spark-submit               |
| ğŸ¦ **Zookeeper** | `2181`  |   TCP    | ZK clients                 |

---

## ğŸ› ï¸ CLI Access

### ğŸ“ 1. Accessing HDFS

```bash
# Enter the NameNode container
docker exec -it namenode bash
```

```bash
# Inside container - HDFS commands
hdfs dfs -ls /
hdfs dfs -mkdir /user
hdfs dfs -mkdir /test_input
hdfs dfs -put localfile.txt /test_input/
```

---

### ğŸ 2. Accessing Hive (SQL)

```bash
# Enter the Hive Server container
docker exec -it hive-server bash
```

```bash
# Inside container - Connect via Beeline
/opt/hive/bin/beeline -u jdbc:hive2://localhost:10000
```

```sql
-- Example Hive commands
SHOW DATABASES;
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE test (id INT, name STRING);
```

> **ğŸ“ Note:** It may take a moment for HiveServer2 to be ready to accept connections after startup.

---

### âš¡ 3. Accessing Spark

```bash
# Enter the Spark Master container
docker exec -it spark-master bash
```

```bash
# Inside container - Launch Spark Shell (Scala)
spark-shell --master spark://spark-master:7077

# Or PySpark
pyspark --master spark://spark-master:7077
```

```scala
// Example Spark commands
val data = Seq(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)
rdd.reduce(_ + _)
```

---

### ğŸ“Š 4. Accessing HBase

```bash
# Enter the HBase Master container
docker exec -it hbase-master bash
```

```bash
# Inside container - Launch HBase Shell
hbase shell
```

```ruby
# Example HBase commands
status
list
create 'users', 'info', 'contact'
put 'users', 'user1', 'info:name', 'John Doe'
scan 'users'
```

---

### ğŸ”µ 5. Accessing Cassandra

```bash
# Connect directly to CQL shell
docker exec -it cassandra cqlsh
```

```sql
-- Example CQL commands
DESCRIBE KEYSPACES;
CREATE KEYSPACE testks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
USE testks;
CREATE TABLE users (id UUID PRIMARY KEY, name TEXT);
```

---

## ğŸ“ Configuration Notes

### ğŸ”— Integration

| Component | Storage Backend                     | Coordination           |
| --------- | ----------------------------------- | ---------------------- |
| HBase     | HDFS (`hdfs://namenode:9000/hbase`) | Zookeeper              |
| Hive      | HDFS (`hdfs://namenode:9000`)       | PostgreSQL (Metastore) |
| Spark     | HDFS (`hdfs://namenode:9000`)       | Standalone             |

### ğŸŒ Networking

All containers communicate through the `bigdata-net` Docker bridge network.

### ğŸ’¾ Persistence

| Volume          | Purpose                |
| --------------- | ---------------------- |
| `namenode_data` | HDFS NameNode metadata |
| `datanode_data` | HDFS DataNode blocks   |

> **âš ï¸ Warning:** Running `docker-compose down -v` will **delete all data** in these volumes!

---

## ğŸ”§ Troubleshooting

### âŒ Common Issues

<details>
<summary><strong>ğŸ”´ Container keeps restarting</strong></summary>

Check logs for the specific container:

```bash
docker logs <container_name>
```

Most common causes:

- Insufficient memory allocated to Docker
- Dependency service not ready yet
</details>

<details>
<summary><strong>ğŸ”´ Cannot connect to Hive</strong></summary>

HiveServer2 takes time to initialize. Check if it's ready:

```bash
docker logs hive-server 2>&1 | grep -i "started"
```

</details>

<details>
<summary><strong>ğŸ”´ HDFS in Safe Mode</strong></summary>

Wait for the cluster to exit safe mode, or force it:

```bash
docker exec -it namenode hdfs dfsadmin -safemode leave
```

</details>

<details>
<summary><strong>ğŸ”´ HBase tables not accessible</strong></summary>

Ensure Zookeeper is running and HBase can connect:

```bash
docker logs hbase-master 2>&1 | grep -i "zookeeper"
```

</details>

---

## ğŸ“š Additional Resources

- ğŸ“– [Apache Hadoop Documentation](https://hadoop.apache.org/docs/)
- ğŸ“– [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- ğŸ“– [Apache Hive Documentation](https://cwiki.apache.org/confluence/display/Hive)
- ğŸ“– [Apache HBase Documentation](https://hbase.apache.org/book.html)
- ğŸ“– [Apache Cassandra Documentation](https://cassandra.apache.org/doc/latest/)

---

<p align="center">
  <sub>Made with â¤ï¸ for Big Data enthusiasts</sub>
</p>
