# ğŸ˜ Local Big Data Dev Stack

<p align="center">
  <img src="https://img.shields.io/badge/Hadoop-3.2.1-yellow?style=for-the-badge&logo=apache-hadoop&logoColor=white" alt="Hadoop"/>
  <img src="https://img.shields.io/badge/Spark-3.0.0-orange?style=for-the-badge&logo=apache-spark&logoColor=white" alt="Spark"/>
  <img src="https://img.shields.io/badge/Hive-2.3.2-green?style=for-the-badge&logo=apache-hive&logoColor=white" alt="Hive"/>
  <img src="https://img.shields.io/badge/HBase-1.2.6-red?style=for-the-badge&logo=apache&logoColor=white" alt="HBase"/>
  <img src="https://img.shields.io/badge/Cassandra-4.0-blue?style=for-the-badge&logo=apache-cassandra&logoColor=white" alt="Cassandra"/>
  <img src="https://img.shields.io/badge/Neo4j-5.15-008CC1?style=for-the-badge&logo=neo4j&logoColor=white" alt="Neo4j"/>
  <img src="https://img.shields.io/badge/Kafka-2.4.1-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white" alt="Kafka"/>
  <img src="https://img.shields.io/badge/Pig-0.17-pink?style=for-the-badge&logo=apache&logoColor=white" alt="Pig"/>
  <img src="https://img.shields.io/badge/Flume-1.9-lightblue?style=for-the-badge&logo=apache&logoColor=white" alt="Flume"/>
  <img src="https://img.shields.io/badge/Oozie-4.3.0-purple?style=for-the-badge&logo=apache&logoColor=white" alt="Oozie"/>
  <img src="https://img.shields.io/badge/Hue-4.10.0-cyan?style=for-the-badge&logo=cloudera&logoColor=white" alt="Hue"/>
  <img src="https://img.shields.io/badge/Sqoop-1.4-brown?style=for-the-badge&logo=apache&logoColor=white" alt="Sqoop"/>
</p>

<p align="center">
  <strong>A modular Big Data ecosystem orchestrated with Docker Compose</strong><br>
  <em>Start only what you need â€¢ Hadoop â€¢ Hive â€¢ HBase â€¢ Spark â€¢ Kafka â€¢ Cassandra â€¢ Neo4j â€¢ Pig â€¢ Flume â€¢ Oozie â€¢ Hue â€¢ Sqoop</em>
</p>

---

## ğŸ“‹ Table of Contents

- [Prerequisites](#-prerequisites)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Architecture](#-architecture)
- [Access Points & UIs](#-access-points--uis)
- [CLI Access](#-cli-access)
- [Configuration Notes](#-configuration-notes)
- [Troubleshooting](#-troubleshooting)

---

## âš ï¸ Prerequisites

|    Requirement    | Minimum | Recommended |
| :---------------: | :-----: | :---------: |
|     ğŸ³ Docker     | Latest  |   Latest    |
| ğŸ”§ Docker Compose |  v2.0+  |    v2.0+    |
|      ğŸ’¾ RAM       | 4-6 GB  |   8-16 GB   |
|      ğŸ–¥ï¸ CPUs      | 2 cores |  4+ cores   |

> **ğŸ’¡ Tip:** With the modular setup, you can now run individual services with much lower resource requirements!

---

## ğŸ“ Project Structure

```
BigData_Docker/
â”œâ”€â”€ ğŸ“„ README.md
â”‚
â”œâ”€â”€ ğŸ“‚ AIO/                     # ğŸš€ All-In-One (Full Stack)
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ pig.Dockerfile
â”‚
â”œâ”€â”€ ğŸ“‚ hadoop/                  # HDFS Cluster
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ zookeeper/               # Coordination Service
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ hive/                    # Data Warehousing
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ spark/                   # Distributed Processing
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ hbase/                   # NoSQL Column Store
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ cassandra/               # NoSQL Wide-Column
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ neo4j/                   # Graph Database
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ kafka/                   # Message Streaming
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ pig/                     # Data Flow Scripting
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ ğŸ“‚ flume/                   # Log/Data Ingestion
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ ğŸ“‚ oozie/                   # Workflow Scheduler
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“‚ hue/                     # Web UI for Hadoop
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â””â”€â”€ ğŸ“‚ sqoop/                   # Data Transfer Tool
    â”œâ”€â”€ docker-compose.yml
    â””â”€â”€ Dockerfile
```

---

## ğŸš€ Quick Start

### ğŸ¯ Option 1: Start Individual Services (Recommended)

Start only the services you need with minimal resources:

|     Service      |                Command                 |    Dependencies    |   RAM   |
| :--------------: | :------------------------------------: | :----------------: | :-----: |
|  ğŸ—‚ï¸ **Hadoop**   |  `cd hadoop && docker-compose up -d`   |        None        |  ~2 GB  |
| ğŸ¦ **Zookeeper** | `cd zookeeper && docker-compose up -d` |        None        | ~512 MB |
|   âš¡ **Spark**   |   `cd spark && docker-compose up -d`   |       Hadoop       |  ~2 GB  |
|   ğŸ **Hive**    |   `cd hive && docker-compose up -d`    |       Hadoop       |  ~2 GB  |
|   ğŸ“Š **HBase**   |   `cd hbase && docker-compose up -d`   | Hadoop + Zookeeper |  ~1 GB  |
| ğŸ”µ **Cassandra** | `cd cassandra && docker-compose up -d` | None (standalone)  |  ~1 GB  |
|   ğŸ•¸ï¸ **Neo4j**   |   `cd neo4j && docker-compose up -d`   | None (standalone)  |  ~1 GB  |
|   ğŸ“¨ **Kafka**   |   `cd kafka && docker-compose up -d`   |     Zookeeper      |  ~1 GB  |
|    ğŸ· **Pig**    |    `cd pig && docker-compose up -d`    |       Hadoop       | ~512 MB |
|   ğŸŒŠ **Flume**   |   `cd flume && docker-compose up -d`   |       Hadoop       | ~512 MB |
|   ğŸ“… **Oozie**   |   `cd oozie && docker-compose up -d`   |       Hadoop       |  ~1 GB  |
|    ğŸ¨ **Hue**    |    `cd hue && docker-compose up -d`    |       Hadoop       |  ~1 GB  |
|   ğŸ”„ **Sqoop**   |   `cd sqoop && docker-compose up -d`   |       Hadoop       | ~512 MB |

#### ğŸ“‹ Example: Start Hadoop + Spark

```bash
# 1. Start Hadoop first (creates the network)
cd hadoop && docker-compose up -d

# 2. Wait for Hadoop to be ready (~1 min)
docker logs -f namenode  # Wait until "Safe mode is OFF"

# 3. Start Spark
cd ../spark && docker-compose up -d
```

#### ğŸ“‹ Example: Start HBase Stack

```bash
# 1. Start Hadoop (creates the network)
cd hadoop && docker-compose up -d

# 2. Start Zookeeper
cd ../zookeeper && docker-compose up -d

# 3. Start HBase
cd ../hbase && docker-compose up -d
```

---

### ğŸŒ Option 2: All-In-One (Full Stack)

Start **all services at once** using the `AIO` folder (requires 10-12 GB RAM):

```bash
# Navigate to the AIO folder
cd AIO

# Build and start all services
docker-compose up -d --build
```

> **ğŸ“¦ What's included in AIO:**
>
> - Hadoop (NameNode + DataNode)
> - Zookeeper
> - Hive (Metastore + Server + PostgreSQL)
> - Spark (Master + Worker)
> - HBase (Master + RegionServer)
> - Cassandra
> - Neo4j
> - Pig

â³ Allow **2-3 minutes** for all services to initialize:

- Hadoop SafeMode exit
- Hive Metastore schema initialization
- HBase region assignment

### âœ… Verify Services

```bash
docker-compose ps

# Or check specific service
cd hadoop && docker-compose ps
```

### ğŸ›‘ Stop Services

```bash
# Stop individual service (preserves data)
cd hadoop && docker-compose down

# Stop all services from root
docker-compose down

# Stop and remove all data (âš ï¸ destructive)
docker-compose down -v
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        bigdata-net (Docker Network)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Zookeeper   â”‚     â”‚   NameNode   â”‚     â”‚   DataNode   â”‚     â”‚
â”‚  â”‚    :2181     â”‚     â”‚    :9870     â”‚     â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚         â–¼                    â–¼                    â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ HBase Master â”‚     â”‚           HDFS Cluster          â”‚       â”‚
â”‚  â”‚   :16010     â”‚â—„â”€â”€â”€â”€â”‚                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                           â”‚                            â”‚
â”‚         â–¼                           â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   HBase      â”‚     â”‚    Hive      â”‚     â”‚    Spark     â”‚     â”‚
â”‚  â”‚ RegionServer â”‚     â”‚   Server     â”‚     â”‚   Master     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   :10000     â”‚     â”‚    :8080     â”‚     â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                    â”‚              â”‚
â”‚                              â–¼                    â–¼              â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                       â”‚  PostgreSQL  â”‚     â”‚    Spark     â”‚     â”‚
â”‚                       â”‚  (Metastore) â”‚     â”‚   Worker     â”‚     â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    :8081     â”‚     â”‚
â”‚                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚  Cassandra   â”‚     â”‚    Neo4j     â”‚  (Standalone DBs)        â”‚
â”‚  â”‚    :9042     â”‚     â”‚    :7474     â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚     Pig      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HDFS                 â”‚
â”‚  â”‚  (Scripting) â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚    Flume     â”‚     â”‚    Oozie     â”‚     â”‚     Hue      â”‚     â”‚
â”‚  â”‚  (Ingestion) â”‚     â”‚  (Workflow)  â”‚     â”‚   (Web UI)   â”‚     â”‚
â”‚  â”‚              â”‚     â”‚   :11000     â”‚     â”‚    :8888     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                    â”‚                    â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                              â–¼                                   â”‚
â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                       â”‚    Sqoop     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–º RDBMS           â”‚
â”‚                       â”‚  (Transfer)  â”‚                          â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ Access Points & UIs

### ğŸŒ Web Interfaces

|   Service    |  Component  |                       URL                        | Description                 |
| :----------: | :---------: | :----------------------------------------------: | :-------------------------- |
| ğŸ—‚ï¸ **HDFS**  | NameNode UI |  [http://localhost:9870](http://localhost:9870)  | Browse the file system      |
| âš¡ **Spark** |  Master UI  |  [http://localhost:8080](http://localhost:8080)  | View Spark jobs & workers   |
| âš¡ **Spark** |  Worker UI  |  [http://localhost:8081](http://localhost:8081)  | View worker details         |
| ğŸ“Š **HBase** |  Master UI  | [http://localhost:16010](http://localhost:16010) | View HBase tables & regions |
| ğŸ•¸ï¸ **Neo4j** |   Browser   |  [http://localhost:7474](http://localhost:7474)  | Graph database browser      |
| ğŸ“… **Oozie** | Web Console | [http://localhost:11000](http://localhost:11000) | Workflow management         |
|  ğŸ¨ **Hue**  |   Browser   |  [http://localhost:8888](http://localhost:8888)  | Hadoop Web UI               |

### ğŸ”— Connection Ports

|     Service      |  Port   | Protocol | Connect With               |
| :--------------: | :-----: | :------: | :------------------------- |
|   ğŸ **Hive**    | `10000` |   JDBC   | Beeline, DBeaver, DataGrip |
|   ğŸ **Hive**    | `10002` |   HTTP   | Web UI                     |
| ğŸ”µ **Cassandra** | `9042`  |   CQL    | cqlsh, drivers             |
|   ğŸ—‚ï¸ **HDFS**    | `9000`  |   RPC    | Hadoop clients             |
|   âš¡ **Spark**   | `7077`  |   RPC    | spark-submit               |
| ğŸ¦ **Zookeeper** | `2181`  |   TCP    | ZK clients                 |
|   ğŸ“¨ **Kafka**   | `9092`  |   TCP    | Kafka clients, producers   |
|   ğŸ•¸ï¸ **Neo4j**   | `7687`  |   Bolt   | Cypher Shell, drivers      |
|   ğŸ“… **Oozie**   | `11000` |   HTTP   | REST API, Web UI           |
|    ğŸ¨ **Hue**    | `8888`  |   HTTP   | Web browser                |

---

## ğŸ› ï¸ CLI Access

### ğŸ“ 1. Accessing HDFS

```bash
# Enter the NameNode container
docker exec -it namenode bash
```

```bash
# Inside container - HDFS commands
hdfs dfs -ls /
hdfs dfs -mkdir /user
hdfs dfs -mkdir /test_input
hdfs dfs -put localfile.txt /test_input/
```

---

### ğŸ 2. Accessing Hive (SQL)

```bash
# Enter the Hive Server container
docker exec -it hive-server bash
```

```bash
# Inside container - Connect via Beeline
/opt/hive/bin/beeline -u jdbc:hive2://localhost:10000
```

```sql
-- Example Hive commands
SHOW DATABASES;
CREATE DATABASE testdb;
USE testdb;
CREATE TABLE test (id INT, name STRING);
```

> **ğŸ“ Note:** It may take a moment for HiveServer2 to be ready to accept connections after startup.

---

### âš¡ 3. Accessing Spark

```bash
# Enter the Spark Master container
docker exec -it spark-master bash
```

```bash
# Inside container - Launch Spark Shell (Scala)
spark-shell --master spark://spark-master:7077

# Or PySpark
pyspark --master spark://spark-master:7077
```

```scala
// Example Spark commands
val data = Seq(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)
rdd.reduce(_ + _)
```

---

### ğŸ“Š 4. Accessing HBase

```bash
# Enter the HBase Master container
docker exec -it hbase-master bash
```

```bash
# Inside container - Launch HBase Shell
hbase shell
```

```ruby
# Example HBase commands
status
list
create 'users', 'info', 'contact'
put 'users', 'user1', 'info:name', 'John Doe'
scan 'users'
```

---

### ğŸ”µ 5. Accessing Cassandra

```bash
# Connect directly to CQL shell
docker exec -it cassandra cqlsh
```

```sql
-- Example CQL commands
DESCRIBE KEYSPACES;
CREATE KEYSPACE testks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
USE testks;
CREATE TABLE users (id UUID PRIMARY KEY, name TEXT);
```

---

### ğŸ•¸ï¸ 6. Accessing Neo4j

```bash
# Open browser UI
# Navigate to http://localhost:7474
# Default credentials: neo4j / password123
```

```bash
# Or use cypher-shell from container
docker exec -it neo4j cypher-shell -u neo4j -p password123
```

```cypher
// Example Cypher commands
CREATE (p:Person {name: 'John', age: 30});
CREATE (p:Person {name: 'Jane', age: 25});
MATCH (a:Person {name: 'John'}), (b:Person {name: 'Jane'})
CREATE (a)-[:KNOWS]->(b);
MATCH (n) RETURN n;
```

---

### ğŸ· 9. Accessing Pig

```bash
# Enter interactive Pig shell (Grunt)
docker exec -it pig pig
```

```bash
# Or run in local mode (no HDFS required)
docker exec -it pig pig -x local
```

```pig
-- Example Pig Latin commands
data = LOAD '/test_input/data.txt' USING PigStorage(',') AS (id:int, name:chararray);
filtered = FILTER data BY id > 10;
grouped = GROUP filtered BY name;
counts = FOREACH grouped GENERATE group, COUNT(filtered);
DUMP counts;
```

> **ğŸ“ Note:** For HDFS mode, ensure Hadoop is running first. Use `-x local` for standalone testing.

---

### ğŸ“¨ 8. Accessing Kafka

```bash
# Enter the Kafka container
docker exec -it kafka bash
```

```bash
# Create a topic
kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9093 --partitions 1 --replication-factor 1

# List topics
kafka-topics.sh --list --bootstrap-server localhost:9093

# Produce messages
kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9093

# Consume messages (in another terminal)
kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9093
```

> **ğŸ“ Note:** Use port `9093` inside the container, `9092` from your host machine.

---

### ğŸŒŠ 9. Accessing Flume

```bash
# Enter the Flume container
docker exec -it flume bash
```

```bash
# Flume is typically configured via agent configuration files
# Example: Start an agent with a specific config
flume-ng agent --conf conf --conf-file /path/to/flume.conf --name agent1 -Dflume.root.logger=INFO,console
```

> **ğŸ“ Note:** Flume requires configuration files for sources, channels, and sinks. Mount your config files when starting the container.

---

### ğŸ“… 10. Accessing Oozie

```bash
# Enter the Oozie container
docker exec -it oozie bash
```

```bash
# Check Oozie status
oozie admin -status

# Submit a workflow
oozie job -oozie http://localhost:11000/oozie -config job.properties -run

# Check job status
oozie job -oozie http://localhost:11000/oozie -info <job-id>
```

```bash
# Access Oozie Web UI
# Navigate to http://localhost:11000/oozie
```

> **ğŸ“ Note:** Oozie requires workflow definitions (XML) and job properties files.

---

### ğŸ¨ 11. Accessing Hue

```bash
# Access Hue Web UI
# Navigate to http://localhost:8888
# Create admin account on first login
```

Hue provides a web interface for:

- ğŸ“ HDFS file browser
- ğŸ Hive query editor
- ğŸ“Š HBase browser
- ğŸ“… Oozie workflow editor
- ğŸ“¨ Kafka topics viewer

> **ğŸ“ Note:** Configure Hue to connect to your Hadoop services via environment variables.

---

### ğŸ”„ 12. Accessing Sqoop

```bash
# Enter the Sqoop container
docker exec -it sqoop bash
```

```bash
# Import data from MySQL to HDFS
sqoop import \
  --connect jdbc:mysql://mysql-host:3306/database \
  --username user --password pass \
  --table tablename \
  --target-dir /user/sqoop/tablename

# Export data from HDFS to MySQL
sqoop export \
  --connect jdbc:mysql://mysql-host:3306/database \
  --username user --password pass \
  --table tablename \
  --export-dir /user/sqoop/tablename

# List databases
sqoop list-databases --connect jdbc:mysql://mysql-host:3306/ --username user --password pass
```

> **ğŸ“ Note:** Sqoop requires JDBC drivers for the target database. Ensure Hadoop is running for HDFS operations.

---

## ğŸ“ Configuration Notes

### ğŸ”— Service Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVICE DEPENDENCY MAP                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Spark   â”‚                        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Hive    â”‚                        â”‚
â”‚        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚        â”‚                                                    â”‚
â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚        â””â”€â”€â”‚   HBase   â”‚â—„â”€â”€â”€â”€â”                               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                               â”‚
â”‚                             â”‚                               â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                               â”‚
â”‚           â”‚ Zookeeper â”‚â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ Cassandra â”‚  (Standalone - no dependencies)              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚   Neo4j   â”‚  (Standalone - no dependencies)              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Zookeeper â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Kafka   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Pig    â”‚                        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Flume   â”‚                        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Oozie   â”‚                        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Hue    â”‚                        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Hadoop   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Sqoop   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º RDBMS        â”‚
â”‚  â”‚  (HDFS)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Start Order by Use Case

| Use Case               | Start Order                      |
| :--------------------- | :------------------------------- |
| ğŸ”¥ **Spark Jobs**      | `hadoop` â†’ `spark`               |
| ğŸ **Hive Queries**    | `hadoop` â†’ `hive`                |
| ğŸ“Š **HBase Tables**    | `hadoop` â†’ `zookeeper` â†’ `hbase` |
| ğŸ”µ **Cassandra**       | `cassandra` (standalone)         |
| ğŸ•¸ï¸ **Neo4j Graphs**    | `neo4j` (standalone)             |
| ğŸ“¨ **Kafka Streams**   | `zookeeper` â†’ `kafka`            |
| ğŸ· **Pig Scripts**     | `hadoop` â†’ `pig`                 |
| ğŸŒŠ **Flume Ingestion** | `hadoop` â†’ `flume`               |
| ğŸ“… **Oozie Workflows** | `hadoop` â†’ `oozie`               |
| ğŸ¨ **Hue Web UI**      | `hadoop` â†’ `hue`                 |
| ğŸ”„ **Sqoop Transfer**  | `hadoop` â†’ `sqoop`               |
| ğŸŒ **Full Stack**      | Root `docker-compose.yml`        |

### ğŸ”— Integration Details

| Component |           Storage Backend           |      Coordination      |
| :-------: | :---------------------------------: | :--------------------: |
|   HBase   | HDFS (`hdfs://namenode:9000/hbase`) |       Zookeeper        |
|   Hive    |    HDFS (`hdfs://namenode:9000`)    | PostgreSQL (Metastore) |
|   Spark   |    HDFS (`hdfs://namenode:9000`)    |       Standalone       |

### ğŸŒ Networking

All services share the `bigdata-net` Docker bridge network. The network is created by the first service you start (Hadoop or Cassandra).

### ğŸ’¾ Persistence

|  Service  |        Volume        | Purpose                |
| :-------: | :------------------: | :--------------------- |
|  Hadoop   |   `namenode_data`    | HDFS NameNode metadata |
|  Hadoop   |   `datanode_data`    | HDFS DataNode blocks   |
| Zookeeper |   `zookeeper_data`   | ZK transaction logs    |
|   Hive    | `hive_postgres_data` | Metastore database     |
| Cassandra |   `cassandra_data`   | Cassandra data files   |
|   Neo4j   |     `neo4j_data`     | Graph database files   |
|    Pig    |    `pig_scripts`     | Pig Latin scripts      |

> **âš ï¸ Warning:** Running `docker-compose down -v` will **delete all data** in these volumes!

---

## ğŸ”§ Troubleshooting

### âŒ Common Issues

<details>
<summary><strong>ğŸ”´ Container keeps restarting</strong></summary>

Check logs for the specific container:

```bash
docker logs <container_name>
```

Most common causes:

- Insufficient memory allocated to Docker
- Dependency service not ready yet
</details>

<details>
<summary><strong>ğŸ”´ Cannot connect to Hive</strong></summary>

HiveServer2 takes time to initialize. Check if it's ready:

```bash
docker logs hive-server 2>&1 | grep -i "started"
```

</details>

<details>
<summary><strong>ğŸ”´ HDFS in Safe Mode</strong></summary>

Wait for the cluster to exit safe mode, or force it:

```bash
docker exec -it namenode hdfs dfsadmin -safemode leave
```

</details>

<details>
<summary><strong>ğŸ”´ HBase tables not accessible</strong></summary>

Ensure Zookeeper is running and HBase can connect:

```bash
docker logs hbase-master 2>&1 | grep -i "zookeeper"
```

</details>

---

## ğŸ“š Additional Resources

- ğŸ“– [Apache Hadoop Documentation](https://hadoop.apache.org/docs/)
- ğŸ“– [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- ğŸ“– [Apache Hive Documentation](https://cwiki.apache.org/confluence/display/Hive)
- ğŸ“– [Apache HBase Documentation](https://hbase.apache.org/book.html)
- ğŸ“– [Apache Cassandra Documentation](https://cassandra.apache.org/doc/latest/)
- ğŸ“– [Neo4j Documentation](https://neo4j.com/docs/)
- ğŸ“– [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- ğŸ“– [Apache Pig Documentation](https://pig.apache.org/docs/latest/)
- ğŸ“– [Apache Flume Documentation](https://flume.apache.org/documentation.html)
- ğŸ“– [Apache Oozie Documentation](https://oozie.apache.org/docs/)
- ğŸ“– [Hue Documentation](https://docs.gethue.com/)
- ğŸ“– [Apache Sqoop Documentation](https://sqoop.apache.org/docs/)

---

<p align="center">
  <sub>Made with â¤ï¸ for Big Data enthusiasts</sub>
</p>
